# 문서내용 - LangChain + Next.js Starter
[LangChain + Next.js Starter – Vercel](https://vercel.com/new/hdh3296s-projects/templates/next.js/langchain-starter)

# LangChain + Next.js Starter
Starter template and example use-cases for LangChain projects in Next.js, including chat, agents, and retrieval.
[Deploy](https://vercel.com/new/hdh3296s-projects/clone?demo-description=Starter%20template%20and%20example%20use-cases%20for%20LangChain%20projects%20in%20Next.js%2C%20including%20chat%2C%20agents%2C%20and%20retrieval.&demo-image=%2F%2Fimages.ctfassets.net%2Fe5382hct74si%2FYIncQSJBMYVkHrvhye3DF%2F5ebf90c898938c30ac1d10c72091968c%2Flangchain-nextjs-template.vercel.app_agents_-_Jacob_Lee.png&demo-title=LangChain%20%2B%20Next.js%20Starter&demo-url=https%3A%2F%2Flangchain-nextjs-template.vercel.app%2F&env=OPENAI_API_KEY&envDescription=Get%20your%20OpenAI%20API%20key%20here%3A&envLink=https%3A%2F%2Fplatform.openai.com%2Faccount%2Fapi-keys&from=templates&project-name=LangChain%20%2B%20Next.js%20Starter&repository-name=langchain-starter&repository-url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain-nextjs-template&skippable-integrations=1)
[View Demo](https://langchain-nextjs-template.vercel.app/)

# LangChain + Next.js 스타터
채팅, 에이전트, 검색을 포함한 Next.js의 LangChain 프로젝트를 위한 스타터 템플릿과 사용 사례 예시입니다.
[배포](https://vercel.com/new/hdh3296s-projects/clone?demo-description=Starter%20template%20and%20example%20use-cases%20for%20LangChain%20projects%20in%20Next.js%2C%20including%20chat%2C%20agents%2C%20and%20retrieval.&demo-image=%2F%2Fimages.ctfassets.net%2Fe5382hct74si%2FYIncQSJBMYVkHrvhye3DF%2F5ebf90c898938c30ac1d10c72091968c%2Flangchain-nextjs-template.vercel.app_agents_-_Jacob_Lee.png&demo-title=LangChain%20%2B%20Next.js%20Starter&demo-url=https%3A%2F%2Flangchain-nextjs-template.vercel.app%2F&env=OPENAI_API_KEY&envDescription=Get%20your%20OpenAI%20API%20key%20here%3A&envLink=https%3A%2F%2Fplatform.openai.com%2Faccount%2Fapi-keys&from=templates&project-name=LangChain%20%2B%20Next.js%20Starter&repository-name=langchain-starter&repository-url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain-nextjs-template&skippable-integrations=1)
[데모 보기](https://langchain-nextjs-template.vercel.app/)

Framework[Next.js](https://vercel.com/templates/next.js)
Use Case[AI](https://vercel.com/templates/ai)[Starter](https://vercel.com/templates/starter)
CSS[Tailwind](https://vercel.com/templates/tailwind)
Database[Supabase](https://vercel.com/templates/supabase)

프레임워크[Next.js](https://vercel.com/templates/next.js)
사용 사례[AI](https://vercel.com/templates/ai)[스타터](https://vercel.com/templates/starter)
CSS[Tailwind](https://vercel.com/templates/tailwind)
데이터베이스[Supabase](https://vercel.com/templates/supabase)

![image](https://vercel.com/_next/image?url=https://images.ctfassets.net/e5382hct74si/YIncQSJBMYVkHrvhye3DF/5ebf90c898938c30ac1d10c72091968c/langchain-nextjs-template.vercel.app_agents_-_Jacob_Lee.png&w=3840&q=75&dpl=dpl_DN6Mz1CkCjSZp3rHysMR9Jkcoysb)
## 🦜️🔗 LangChain + Next.js Starter Template
[Link](https://codespaces.new/langchain-ai/langchain-nextjs-template)[Link](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain-nextjs-template)
This template scaffolds a LangChain.js + Next.js starter app. It showcases how to use and combine LangChain modules for several use cases. Specifically:
- [Simple chat](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [Returning structured output from an LLM call](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [Answering complex, multi-step questions with agents](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [Retrieval augmented generation (RAG) with a chain and a vector store](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [Retrieval augmented generation (RAG) with an agent and a vector store](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)

Most of them use Vercel's [AI SDK](https://github.com/vercel-labs/ai) to stream tokens to the client and display the incoming messages.
The agents use [LangGraph.js](https://langchain-ai.github.io/langgraphjs/), LangChain's framework for building agentic workflows. They use preconfigured helper functions to minimize boilerplate, but you can replace them with custom graphs as desired.

## 🦜️🔗 LangChain + Next.js 스타터 템플릿
[링크](https://codespaces.new/langchain-ai/langchain-nextjs-template)[링크](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain-nextjs-template)
이 템플릿은 LangChain.js + Next.js 스타터 앱을 구축합니다. 여러 사용 사례에 대해 [[LangChain 모듈]]을 사용하고 결합하는 방법을 보여줍니다. 구체적으로:
- [간단한 채팅](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [LLM 호출에서 구조화된 출력 반환](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [에이전트를 사용한 복잡한 다단계 질문 답변](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [체인과 벡터 저장소를 사용한 검색 증강 생성 (RAG)](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)
- [에이전트와 벡터 저장소를 사용한 검색 증강 생성 (RAG)](https://vercel.com/github.com/langchain-ai/langchain-nextjs-template/tree/main/)

대부분은 Vercel의 [AI SDK](https://github.com/vercel-labs/ai)를 사용하여 클라이언트에 토큰을 스트리밍하고 수신 메시지를 표시합니다.
에이전트는 LangChain의 에이전트 워크플로우 구축 프레임워크인 [LangGraph.js](https://langchain-ai.github.io/langgraphjs/)를 사용합니다. 보일러플레이트를 최소화하기 위해 사전 구성된 헬퍼 함수를 사용하지만, 원하는 대로 사용자 정의 그래프로 대체할 수 있습니다.

It's free-tier friendly too! Check out the bundle size stats below.
You can check out a hosted version of this repo here: [https://langchain-nextjs-template.vercel.app/](https://langchain-nextjs-template.vercel.app/)

무료 티어에도 친화적입니다! 아래의 번들 크기 통계를 확인해보세요.
이 리포지토리의 호스팅된 버전은 여기에서 확인할 수 있습니다: [https://langchain-nextjs-template.vercel.app/](https://langchain-nextjs-template.vercel.app/)

### 🚀 시작하기
이 지침은 개발자가 프로젝트를 설정하고 개발을 시작하는 
데 필요한 단계를 안내합니다.
> 
> 1. **레포지토리 클론**: 먼저, 해당 프로젝트의 레포지토리를 클론하여 로컬 컴퓨터에 다운로드합니다.
> 2. **환경 변수 설정**: 로컬 레포지토리의 `.env.local` 파일에 환경 변수를 설정해야 합니다. `.env.example` 파일을 복사하여 `.env.local`로 만들고, OpenAI API 키를 추가합니다.
> 3. **서버리스 Edge 함수 설정**: 이 애플리케이션은 서버리스 Edge 함수에서 실행되도록 설계되었으므로, `LANGCHAIN_CALLBACKS_BACKGROUND` 환경 변수를 false로 설정해야 LangSmith 추적이 완료됩니다.
> 4. **필요한 패키지 설치**: 선호하는 패키지 관리자를 사용하여 필요한 패키지를 설치합니다. 예를 들어, `yarn`을 사용할 수 있습니다.
> 5. **개발 서버 실행**: `yarn dev` 명령어를 통해 개발 서버를 실행할 수 있습니다. 브라우저에서 [http://localhost:3000](http://localhost:3000/)을 열어 결과를 확인하세요! 봇에게 무언가를 물어보면 스트리밍 응답을 볼 수 있습니다:
> 6. **페이지 수정**: `app/page.tsx` 파일을 수정하여 페이지를 편집할 수 있으며, 파일을 편집할 때마다 페이지가 자동으로 업데이트됩니다.
> 7. **백엔드 로직**: 백엔드 로직은 `app/api/chat/route.ts`에 위치하며, 여기에서 프롬프트와 모델을 변경하거나 다른 모듈과 로직을 추가할 수 있습니다. 


### 🧱 Structured Output
The second example shows how to have a model return output according to a specific schema using OpenAI Functions. Click the Structured Output link in the navbar to try it out:

### 🧱 구조화된 출력
두 번째 예제는 OpenAI Functions를 사용하여 특정 스키마에 따라 모델이 출력을 반환하는 방법을 보여줍니다. 네비게이션 바에서 구조화된 출력 링크를 클릭하여 시도해보세요:

The chain in this example uses a [popular library called Zod](https://zod.dev/) to construct a schema, then formats it in the way OpenAI expects. It then passes that schema as a function into OpenAI and passes a function_call parameter to force OpenAI to return arguments in the specified format.
For more details, [check out this documentation page](https://js.langchain.com/docs/how_to/structured_output).

이 예제의 체인은 [Zod라는 인기 있는 라이브러리](https://zod.dev/)를 사용하여 스키마를 구성한 다음, OpenAI가 예상하는 방식으로 포맷합니다. 그런 다음 해당 스키마를 함수로 OpenAI에 전달하고 function_call 매개변수를 전달하여 OpenAI가 지정된 형식으로 인수를 반환하도록 강제합니다.
자세한 내용은 [이 문서 페이지를 확인하세요](https://js.langchain.com/docs/how_to/structured_output).

### 🦜 Agents
To try out the agent example, you'll need to give the agent access to the internet by populating the SERPAPI_API_KEY in .env.local. Head over to [the SERP API website](https://serpapi.com/) and get an API key if you don't already have one.
You can then click the Agent example and try asking it more complex questions:

### 🦜 에이전트
에이전트 예제를 시도해보려면 .env.local에 SERPAPI_API_KEY를 입력하여 에이전트에 인터넷 접근 권한을 부여해야 합니다. [SERP API 웹사이트](https://serpapi.com/)로 이동하여 API 키가 없다면 하나를 받으세요.
그런 다음 에이전트 예제를 클릭하고 더 복잡한 질문을 해볼 수 있습니다:

This example uses a [prebuilt LangGraph agent](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/), but you can customize your own as well.

이 예제는 [사전 구축된 LangGraph 에이전트](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/)를 사용하지만, 자신만의 에이전트를 커스터마이즈할 수도 있습니다.

### 🐶 Retrieval
The retrieval examples both use Supabase as a vector store. However, you can swap in [another supported vector store](https://js.langchain.com/docs/integrations/vectorstores) if preferred by changing the code under app/api/retrieval/ingest/route.ts, app/api/chat/retrieval/route.ts, and app/api/chat/retrieval_agents/route.ts.
For Supabase, follow [these instructions](https://js.langchain.com/docs/integrations/vectorstores/supabase) to set up your database, then get your database URL and private key and paste them into .env.local.
You can then switch to the Retrieval and Retrieval Agent examples. The default document text is pulled from the LangChain.js retrieval use case docs, but you can change them to whatever text you'd like.
For a given text, you'll only need to press Upload once. Pressing it again will re-ingest the docs, resulting in duplicates. You can clear your Supabase vector store by navigating to the console and running DELETE FROM documents;.
After splitting, embedding, and uploading some text, you're ready to ask questions!

### 🐶 검색
검색 예제들은 모두 Supabase를 벡터 저장소로 사용합니다. 하지만 app/api/retrieval/ingest/route.ts, app/api/chat/retrieval/route.ts, app/api/chat/retrieval_agents/route.ts 아래의 코드를 변경하여 [다른 지원되는 벡터 저장소](https://js.langchain.com/docs/integrations/vectorstores)로 교체할 수 있습니다.
Supabase의 경우, [이 지침](https://js.langchain.com/docs/integrations/vectorstores/supabase)을 따라 데이터베이스를 설정한 다음, 데이터베이스 URL과 개인 키를 가져와 .env.local에 붙여넣으세요.
그런 다음 검색 및 검색 에이전트 예제로 전환할 수 있습니다. 기본 문서 텍스트는 LangChain.js 검색 사용 사례 문서에서 가져온 것이지만, 원하는 텍스트로 변경할 수 있습니다.
주어진 텍스트에 대해 업로드를 한 번만 누르면 됩니다. 다시 누르면 문서를 재수집하여 중복이 발생합니다. Supabase 벡터 저장소를 지우려면 콘솔로 이동하여 DELETE FROM documents;를 실행하세요.
텍스트를 분할, 임베딩, 업로드한 후에는 질문할 준비가 됩니다!

For more info on retrieval chains, [see this page](https://js.langchain.com/docs/tutorials/rag). The specific variant of the conversational retrieval chain used here is composed using LangChain Expression Language, which you can [read more about here](https://js.langchain.com/docs/how_to/qa_sources/). This chain example will also return cited sources via header in addition to the streaming response.
For more info on retrieval agents, [see this page](https://langchain-ai.github.io/langgraphjs/tutorials/rag/langgraph_agentic_rag/).

검색 체인에 대한 자세한 정보는 [이 페이지](https://js.langchain.com/docs/tutorials/rag)를 참조하세요. 여기서 사용된 대화형 검색 체인의 특정 변형은 LangChain Expression Language를 사용하여 구성되었으며, [여기에서 더 자세히 읽을 수 있습니다](https://js.langchain.com/docs/how_to/qa_sources/). 이 체인 예제는 스트리밍 응답 외에도 헤더를 통해 인용된 소스를 반환합니다.
검색 에이전트에 대한 자세한 정보는 [이 페이지](https://langchain-ai.github.io/langgraphjs/tutorials/rag/langgraph_agentic_rag/)를 참조하세요.

### 📦 Bundle size
The bundle size for LangChain itself is quite small. After compression and chunk splitting, for the RAG use case LangChain uses 37.32 KB of code space (as of [@langchain/core 0.1.15](https://npmjs.com/package/@langchain/core)), which is less than 4% of the total Vercel free tier edge function alottment of 1 MB:

### 📦 번들 크기
LangChain 자체의 번들 크기는 매우 작습니다. 압축 및 청크 분할 후, RAG 사용 사례에서 LangChain은 37.32 KB의 코드 공간을 사용합니다([@langchain/core 0.1.15](https://npmjs.com/package/@langchain/core) 기준). 이는 Vercel 무료 티어 엣지 함수 할당량 1 MB의 4% 미만입니다:

This package has [@next/bundle-analyzer](https://www.npmjs.com/package/@next/bundle-analyzer) set up by default - you can explore the bundle size interactively by running:
$ ANALYZE=true yarn build

이 패키지는 기본적으로 [@next/bundle-analyzer](https://www.npmjs.com/package/@next/bundle-analyzer)가 설정되어 있습니다 - 다음 명령을 실행하여 번들 크기를 대화식으로 탐색할 수 있습니다:
$ ANALYZE=true yarn build

### 📚 Learn More
The example chains in the app/api/chat/route.ts and app/api/chat/retrieval/route.ts files use [LangChain Expression Language](https://js.langchain.com/docs/concepts#langchain-expression-language) to compose different LangChain.js modules together. You can integrate other retrievers, agents, preconfigured chains, and more too, though keep in mind HttpResponseOutputParser is meant to be used directly with model output.
To learn more about what you can do with LangChain.js, check out the docs here:
- [https://js.langchain.com/docs/](https://js.langchain.com/docs/)

### 📚 더 알아보기
app/api/chat/route.ts 및 app/api/chat/retrieval/route.ts 파일의 예제 체인은 [LangChain Expression Language](https://js.langchain.com/docs/concepts#langchain-expression-language)를 사용하여 다양한 LangChain.js 모듈을 함께 구성합니다. 다른 검색기, 에이전트, 사전 구성된 체인 등을 통합할 수 있지만, HttpResponseOutputParser는 모델 출력과 직접 사용하도록 설계되었음을 명심하세요.
LangChain.js로 할 수 있는 일에 대해 더 자세히 알아보려면 다음 문서를 확인하세요:
- [https://js.langchain.com/docs/](https://js.langchain.com/docs/)

### ▲ Deploy on Vercel
When ready, you can deploy your app on the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme).
Check out the [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.

### ▲ Vercel에 배포하기
준비가 되면 [Vercel 플랫폼](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme)에 앱을 배포할 수 있습니다.
자세한 내용은 [Next.js 배포 문서](https://nextjs.org/docs/deployment)를 확인하세요.

### Thank You!
Thanks for reading! If you have any questions or comments, reach out to us on Twitter [@LangChainAI](https://twitter.com/langchainai), or [click here to join our Discord server](https://discord.gg/langchain).

### 감사합니다!
읽어주셔서 감사합니다! 질문이나 의견이 있으시면 Twitter [@LangChainAI](https://twitter.com/langchainai)로 연락하거나 [여기를 클릭하여 Discord 서버에 참여](https://discord.gg/langchain)하세요.


----

이 기술문서의 개요
- [[LangChain + Next.js 스타터 개요]]